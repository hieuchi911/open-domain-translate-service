{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.18.0\n",
    "# !pip install sentencepiece==0.1.96\n",
    "# !pip install sanic==22.3.1\n",
    "# !pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Load translation models and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Facebook mbart large\n",
    "multi-language MT model: Facebook mbart model https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First load MT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"models/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"models/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then use model and tokenizer for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(\"Hello, how are you today?\", return_tensors = \"pt\")\n",
    "generated_tokens = model.generate(**model_inputs, forced_bos_token_id = tokenizer.lang_code_to_id [\"vi_VN\"])\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Opus Machine translation En-Vi\n",
    "Model for Eng-Viet translation from Helsinki-NLP https://huggingface.co/Helsinki-NLP/opus-mt-en-vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-26 11:02:54.563276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-26 11:02:54.563395: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3516: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♪ mọi chuyện là như nhau ♪\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Tokenize text\n",
    "text = \"everything is the same\"\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "\n",
    "# Perform translation and decode the output\n",
    "translation = model.generate(**tokenized_text)\n",
    "translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print translated text\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NlpHUST Machine Translation\n",
    "Model for Viet-Eng and Eng-Viet translation from NlpHUST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eng-Viet translation model: https://huggingface.co/Helsinki-NLP/opus-mt-en-vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-30 10:06:43.867673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 10:06:43.867714: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies . \n",
      "--->\t Ở trường, chúng tôi dành nhiều thời gian để nghiên cứu về lịch sử Kim Il-Sung, nhưng chúng tôi chưa bao giờ học được nhiều về thế giới bên ngoài, ngoại trừ Mỹ, Hàn Quốc, Nhật Bản là kẻ thù.\n",
      "\n",
      "\n",
      "I hope you're doing fine \n",
      "--->\t Tôi hi vọng anh đang làm tốt.\n",
      "\n",
      "\n",
      "When the rain keeps falling down. Trouble keeps on coming round \n",
      "--->\t Khi mưa tiếp tục rơi xuống.\n",
      "\n",
      "\n",
      "When the rain keeps falling down, trouble keeps on coming round \n",
      "--->\t Khi mưa tiếp tục rơi xuống, rắc rối tiếp tục đến.\n",
      "\n",
      "\n",
      "If you're hiding what you feel. Underneath a bitter pill \n",
      "--->\t Nếu anh đang che giấu những gì anh cảm thấy.\n",
      "\n",
      "\n",
      "If you're hiding what you feel. underneath a bitter pill \n",
      "--->\t Nếu cô đang che giấu những gì cô cảm thấy. dưới một viên thuốc cay\n",
      "\n",
      "\n",
      "hello \n",
      "--->\t Xin chào.\n",
      "\n",
      "\n",
      "hi \n",
      "--->\t Xin chào.\n",
      "\n",
      "\n",
      "the robots \n",
      "--->\t những con robot\n",
      "\n",
      "\n",
      "the butterflies \n",
      "--->\t những con bướm\n",
      "\n",
      "\n",
      "robots \n",
      "--->\t robots\n",
      "\n",
      "\n",
      "butterfly \n",
      "--->\t Bướm\n",
      "\n",
      "\n",
      "i'm my own boss \n",
      "--->\t tôi là sếp của riêng tôi.\n",
      "\n",
      "\n",
      "i am my own boss \n",
      "--->\t tôi là sếp của riêng tôi.\n",
      "\n",
      "\n",
      "i am ok \n",
      "--->\t tôi ổn.\n",
      "\n",
      "\n",
      "hi, so sweet i can't resist your charm \n",
      "--->\t Xin chào, tôi không thể cưỡng lại sự quyến rũ của cô.\n",
      "\n",
      "\n",
      "don't play with my heart. Before we let it fall too far \n",
      "--->\t Đừng đùa với trái tim tôi trước khi chúng ta để nó rơi quá xa\n",
      "\n",
      "\n",
      "don't play with my heart. before we let it fall too far \n",
      "--->\t Đừng đùa với trái tim tôi trước khi chúng ta để nó rơi quá xa\n",
      "\n",
      "\n",
      "It is a little chilly, but not too bad. how about where you are at? \n",
      "--->\t Nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "It is a little chilly, but not too bad. How about where you are at? \n",
      "--->\t Nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "it is a little chilly, but not too bad. how about where you are at? \n",
      "--->\t nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "hello. i'm doing just fine. how about you? \n",
      "--->\t Xin chào, tôi đang làm rất tốt.\n",
      "\n",
      "\n",
      "hello, i'm doing just fine. How about you? \n",
      "--->\t Xin chào, tôi đang làm tốt.\n",
      "\n",
      "\n",
      "i am right here. But why can't you see. Please scan more! \n",
      "--->\t Tôi đang ở đây, nhưng sao anh không thấy.\n",
      "\n",
      "\n",
      "i am right here. but why can't you see. Please scan more! \n",
      "--->\t Tôi đang ở đây, nhưng sao anh không thấy.\n",
      "\n",
      "\n",
      "One room just ain't enough. But it's hard to leave you alone \n",
      "--->\t Một căn phòng không đủ.\n",
      "\n",
      "\n",
      "One room just ain't enough. but it's hard to leave you alone \n",
      "--->\t Một căn phòng chưa đủ, nhưng khó để cô yên.\n",
      "\n",
      "\n",
      "one room just ain't enough. but it's hard to leave you alone \n",
      "--->\t một căn phòng không đủ, nhưng khó để cô yên.\n",
      "\n",
      "\n",
      "That I know that i'm still free. Be anywhere that I wanna be \n",
      "--->\t Tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "That I know that i'm still free. be anywhere that I wanna be \n",
      "--->\t Tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "that i know that i'm still free. be anywhere that i wanna be \n",
      "--->\t mà tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. When we are apart \n",
      "--->\t Khoảng cách khiến trái tim phát triển. hạnh phúc khi biết rằng tình yêu của anh chưa bao giờ đi xa.\n",
      "\n",
      "\n",
      "Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart \n",
      "--->\t Khoảng cách khiến trái tim phát triển. thật hạnh phúc khi biết rằng tình yêu của anh không bao giờ xa, khi chúng ta chia cắt\n",
      "\n",
      "\n",
      "distance makes the heart grows. even when i'm lonely, happy knowing that your love is never far. when we are apart \n",
      "--->\t khoảng cách làm tim phát triển. thậm chí khi tôi cô đơn, hạnh phúc khi biết rằng tình yêu của anh chưa bao giờ đi xa.\n",
      "\n",
      "\n",
      "When you love. It makes a heart. In the middle of a fight. Walk away to make it right \n",
      "--->\t Khi em yêu, nó là một trái tim.\n",
      "\n",
      "\n",
      "When you love, It makes a heart. In the middle of a fight; Walk away to make it right \n",
      "--->\t Khi em yêu, nó là một trái tim.\n",
      "\n",
      "\n",
      "when you love, it makes a heart. in the middle of a fight; walk away to make it right \n",
      "--->\t khi em yêu, nó là một trái tim. giữa cuộc chiến, bước đi để làm cho nó đúng\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-en-vi-small\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-en-vi-small\")\n",
    "t5_model.to(device)\n",
    "\n",
    "srcs = [\"In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\",\n",
    "       \"I hope you're doing fine\",\n",
    "       \"When the rain keeps falling down. Trouble keeps on coming round\",\n",
    "       \"When the rain keeps falling down, trouble keeps on coming round\",\n",
    "       \"If you're hiding what you feel. Underneath a bitter pill\",\n",
    "       \"If you're hiding what you feel. underneath a bitter pill\",\n",
    "       \"hello\", \"hi\", \"the robots\",\n",
    "       \"the butterflies\", \"robots\", \"butterfly\",\n",
    "       \"i'm my own boss\", \"i am my own boss\",\n",
    "       \"i am ok\",\n",
    "       \"hi, so sweet i can't resist your charm\",\n",
    "       \"don't play with my heart. Before we let it fall too far\",\n",
    "       \"don't play with my heart. before we let it fall too far\",\n",
    "       \"It is a little chilly, but not too bad. how about where you are at?\",\n",
    "       \"It is a little chilly, but not too bad. How about where you are at?\",\n",
    "       \"It is a little chilly, but not too bad. how about Where you are at?\".lower(),\n",
    "       \"hello. i'm doing just fine. how about you?\",\n",
    "       \"hello, i'm doing just fine. How about you?\",\n",
    "       \"i am right here. But why can't you see. Please scan more!\",\n",
    "       \"i am right here. but why can't you see. Please scan more!\",\n",
    "       \"One room just ain't enough. But it's hard to leave you alone\",\n",
    "       \"One room just ain't enough. but it's hard to leave you alone\",\n",
    "       \"One room just ain't enough. but it's hard to leave you alone\".lower(),\n",
    "       \"That I know that i'm still free. Be anywhere that I wanna be\",\n",
    "       \"That I know that i'm still free. be anywhere that I wanna be\",\n",
    "       \"That I know that i'm still free. be anywhere that I wanna be\".lower(),\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. When we are apart\",\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart\",\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart\".lower(),\n",
    "       \"When you love. It makes a heart. In the middle of a fight. Walk away to make it right\",\n",
    "       \"When you love, It makes a heart. In the middle of a fight; Walk away to make it right\",\n",
    "       \"When you love, It makes a heart. In the middle of a fight; Walk away to make it right\".lower(),]\n",
    "for src in srcs:\n",
    "    tokenized_text = t5_tokenizer.encode(src, return_tensors=\"pt\").to(device)\n",
    "    t5_model.eval()\n",
    "    summary_ids = t5_model.generate(\n",
    "                        tokenized_text,\n",
    "                        max_length=128, \n",
    "                        num_beams=5,\n",
    "                        repetition_penalty=2.5, \n",
    "                        length_penalty=1.0, \n",
    "                        early_stopping=True)\n",
    "    output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(f\"{src} \\n--->\\t {output}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Viet-Eng translation model: https://huggingface.co/Helsinki-NLP/opus-mt-vi-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 607/607 [00:00<00:00, 301kB/s]\n",
      "Downloading: 100%|██████████| 1.12G/1.12G [06:05<00:00, 3.29MB/s]   \n",
      "Downloading: 100%|██████████| 4.11M/4.11M [00:02<00:00, 1.68MB/s]\n",
      "Downloading: 100%|██████████| 98.0/98.0 [00:00<00:00, 22.4kB/s]\n",
      "Downloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 23.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin chào ngày mới. thời tiết ở đó thế nào?  ----> \t Hello, new day. How's the weather there?\n",
      "Indonesia phỏng đoán nguyên nhân tàu ngầm chở 53 người mất tích bí ẩn  ----> \t Indonesia anticipates the cause of the submarine transporting 53 mysterious missing persons\n",
      "bạn khỏe không? Rất vui được gặp mặt!  ----> \t How are you?\n",
      "bạn có khỏe không? rất vui được gặp lại bạn!  ----> \t How are you? I'm so happy to see you again!\n",
      "bạn khỏe không? rất vui được gặp mặt!  ----> \t How are you? I'm so happy to meet you!\n",
      "bạn khỏe không? Rất vui được gặp mặt!  ----> \t How are you?\n"
     ]
    }
   ],
   "source": [
    "t5_model_vi = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-vi-en-small\")\n",
    "t5_tokenizer_vi = T5Tokenizer.from_pretrained(\"NlpHUST/t5-vi-en-small\")\n",
    "t5_model_vi.to(device)\n",
    "\n",
    "srcs = [\"xin chào ngày mới. thời tiết ở đó thế nào?\",\n",
    "       \"Indonesia phỏng đoán nguyên nhân tàu ngầm chở 53 người mất tích bí ẩn\",\n",
    "       \"bạn khỏe không? Rất vui được gặp mặt!\",\n",
    "       \"bạn có khỏe không? rất vui được gặp lại bạn!\",\n",
    "       \"bạn khỏe không? rất vui được gặp mặt!\",\n",
    "       \"bạn khỏe không? Rất vui được gặp mặt!\"]\n",
    "for src in srcs:\n",
    "    tokenized_text = t5_tokenizer_vi.encode(src, return_tensors=\"pt\").to(device)\n",
    "    t5_model_vi.eval()\n",
    "    summary_ids = t5_model_vi.generate(\n",
    "                        tokenized_text,\n",
    "                        max_length=256, \n",
    "                        num_beams=5,\n",
    "                        repetition_penalty=2.5, \n",
    "                        length_penalty=1.0, \n",
    "                        early_stopping=True\n",
    "                    )\n",
    "    output = t5_tokenizer_vi.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(src, \" ----> \\t\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Test Sanic Service\n",
    "Following cells tests open-chat-bot-kit: en-vi vi-en services and open-chatbot service\n",
    "- translate services, en-vi vi-en:  port 8000\n",
    "- open-chatbot service listens:     port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy sanic service on local machine by executing:\n",
    "## python -m sanic <app file name, e.g app.py then use 'app'>:<sanic app defined in python file> -H 0.0.0.0 -p <listening port>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 service blocks tested separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how's the weather today?\n"
     ]
    }
   ],
   "source": [
    "# VIETNAMESE-TO-ENGLISH TRANSLATION SERVICE\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url_obj = {'text': \"xin chào, hôm nay thời tiết như thế nào?\"}\n",
    "\n",
    "stt_server = 'http://0.0.0.0:8000/vi-to-en'\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))  # parse the dictionary to json with json.dumps()\n",
    "vi_en = text.text\n",
    "print(vi_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is a little chilly, but not too bad. How about where you are at?\n"
     ]
    }
   ],
   "source": [
    "# OPEN-DOMAIN-CHATBOT SERVICE\n",
    "\n",
    "# pass vi_en from previous cell to open chat service\n",
    "stt_server = 'http://0.0.0.0:8001/open-chat'\n",
    "text_obj = {'text': vi_en}\n",
    "text = requests.post(stt_server, data=json.dumps(text_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "print(bot_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi là sếp của riêng tôi.\n"
     ]
    }
   ],
   "source": [
    "# ENGLISH-TO-VIETNAMESE TRANSLATION SERVICE\n",
    "\n",
    "url_obj = {'text': \"i'm my own boss\"}\n",
    "stt_server = 'http://0.0.0.0:8000/en-to-vi'\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "en_vi = text.text\n",
    "print(en_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 blocks tested in 1 flow (raw input fed into translation models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'text': \"xin chào một ngày tốt lành\".lower()}\n",
    "\n",
    "\n",
    "text = requests.post('http://0.0.0.0:8000/vi-to-en', data=json.dumps(payload))\n",
    "vi_en = text.text\n",
    "\n",
    "text_obj = {'text': vi_en.lower()}\n",
    "text = requests.post('http://0.0.0.0:8001/open-chat', data=json.dumps(text_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "\n",
    "text_obj = {'text': bot_answer.lower()}\n",
    "text = requests.post('http://0.0.0.0:8000/en-to-vi', data=json.dumps(text_obj))\n",
    "en_vi = text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 blocks tested in 1 flow: with preprocessed inputs (break inputs into partitions by punctuations) before fed into MT models for better translation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t###EN-TO-VI\n",
      "[{'text': 'cung hoàng đạo của bạn là gì'}, {'text': '?'}]\n",
      "What is your horoscope?? \n",
      "\t###BLENDER-BOT\n",
      "i don't have a horoscope. i'm not sure what it is.\n",
      "\t###EN-TO-VI\n",
      "[{'text': \"i don't have a horoscope\"}, {'text': '.'}, {'text': \" i'm not sure what it is\"}, {'text': '.'}]\n",
      "tôi không có tử vi. tôi không chắc nó là gì. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"\\t###EN-TO-VI\")\n",
    "msg = \"cung hoàng đạo của bạn là gì?\"\n",
    "url_objs = []\n",
    "\n",
    "# break original input by punctuations, then translate each partition, which gives better translation result\n",
    "for i in re.split(\"([,\\?\\!\\.])\", msg.lower()):\n",
    "    if len(i) > 0:\n",
    "        url_objs.append({'text': i})\n",
    "\n",
    "stt_server = 'http://0.0.0.0:8000/vi-to-en'\n",
    "vi_en = \"\"\n",
    "# translate each partition\n",
    "for i, url_obj in enumerate(url_objs):\n",
    "    if url_obj[\"text\"] not in [\",\", \"?\", \"!\", \".\"]:\n",
    "        text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "        vi_en += text.text\n",
    "    else:\n",
    "        vi_en += url_obj[\"text\"] + \" \"\n",
    "print(vi_en)\n",
    "\n",
    "\n",
    "print(\"\\t###BLENDER-BOT\")\n",
    "stt_server = 'http://0.0.0.0:8001/open-chat'\n",
    "url_obj = {'text': re.sub(r\"[\\?\\!\\.]\", \",\", vi_en.lower())}\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "print(bot_answer)\n",
    "\n",
    "\n",
    "print(\"\\t###EN-TO-VI\")\n",
    "url_objs = []\n",
    "# break original input by punctuations, then translate each partition, which gives better translation result\n",
    "for i in re.split(\"([,\\?\\!\\.])\", bot_answer.lower()):\n",
    "    if len(i) > 0:\n",
    "        url_objs.append({'text': i})\n",
    "print(url_objs)\n",
    "\n",
    "stt_server = 'http://0.0.0.0:8000/en-to-vi'\n",
    "en_vi = \"\"\n",
    "# translate each partition\n",
    "for i, url_obj in enumerate(url_objs):\n",
    "    if url_obj[\"text\"] not in [\",\", \"?\", \"!\", \".\"]:\n",
    "        text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "        en_vi += text.text\n",
    "    else:\n",
    "        en_vi += url_obj[\"text\"] + \" \"\n",
    "print(en_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Test Rasa chatbot eqquiped with Open-domain-kit via REST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 services of open-domain-kit are deployed along with Rasa server (in docker-compose), and will be summoned in action server (handled via out_of_scope intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'test', 'text': 'Lúc nào cũng vui khi có cơ hội ngắm nhìn thời tiết tốt nhất thế giới.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "payload = {\"sender\": \"test\", \"message\": \"dự báo thời tiết hôm nay\"}\n",
    "# open-domain-kit integrated into Rasa at \n",
    "text = requests.post('http://localhost:5005/webhooks/rest/webhook', data=json.dumps(payload))\n",
    "vi_en = text.text\n",
    "print(json.loads(vi_en))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2b9313a5ac1dd114cb80c590fd5994e6bb5ba093af05652baffc6e4ba6c0185"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('nmt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
