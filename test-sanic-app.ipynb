{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.18.0\n",
    "# !pip install sentencepiece\n",
    "# !pip install sanic==22.3.1\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook mbart large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-05-11 10:46:39.100896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 10:46:39.100949: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/hieu/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"models/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"models/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(\"Hello, how are you today?\", return_tensors = \"pt\")\n",
    "generated_tokens = model.generate(**model_inputs, forced_bos_token_id = tokenizer.lang_code_to_id [\"vi_VN\"])\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-12 08:19:43 +0700] [32639] [INFO] \n",
      "  ┌────────────────────────────────────────────────────────────────────────┐\n",
      "  │                             Sanic v22.3.1                              │\n",
      "  │                    Goin' Fast @ http://0.0.0.0:5351                    │\n",
      "  ├───────────────────────┬────────────────────────────────────────────────┤\n",
      "  │ \u001b[48;2;255;13;104m                     \u001b[0m │     mode: production, single worker            │\n",
      "  │ \u001b[38;2;255;255;255;48;2;255;13;104m    ▄███ █████ ██    \u001b[0m │   server: sanic                                │\n",
      "  │ \u001b[38;2;255;255;255;48;2;255;13;104m   ██                \u001b[0m │   python: 3.8.10                               │\n",
      "  │ \u001b[38;2;255;255;255;48;2;255;13;104m    ▀███████ ███▄    \u001b[0m │ platform: Linux-5.13.0-40-generic-x86_64-with- │\n",
      "  │ \u001b[38;2;255;255;255;48;2;255;13;104m                ██   \u001b[0m │           glibc2.29                            │\n",
      "  │ \u001b[38;2;255;255;255;48;2;255;13;104m   ████ ████████▀    \u001b[0m │ packages: sanic-routing==22.3.0                │\n",
      "  │ \u001b[48;2;255;13;104m                     \u001b[0m │                                                │\n",
      "  │ Build Fast. Run Fast. │                                                │\n",
      "  └───────────────────────┴────────────────────────────────────────────────┘\n",
      "\n",
      "[2022-05-12 08:19:43 +0700] [32639] [WARNING] \u001b[01;33mSanic is running in PRODUCTION mode. Consider using '--debug' or '--dev' while actively developing your application.\u001b[0m\n",
      "[2022-05-12 08:19:43 +0700] [32639] [ERROR] Experienced exception while trying to serve\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 596, in _get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/utils/hub.py\", line 282, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/utils/hub.py\", line 470, in get_from_cache\n",
      "    os.makedirs(cache_dir, exist_ok=True)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: 'model/en-vi'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/mixins/runner.py\", line 578, in serve\n",
      "    serve_single(primary_server_info.settings)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/server/runners.py\", line 206, in serve_single\n",
      "    serve(**server_settings)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/server/runners.py\", line 130, in serve\n",
      "    loop.run_until_complete(app._server_event(\"init\", \"before\"))\n",
      "  File \"uvloop/loop.pyx\", line 1501, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/app.py\", line 1581, in _server_event\n",
      "    await self.dispatch(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/signals.py\", line 193, in dispatch\n",
      "    return await dispatch\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/signals.py\", line 163, in _dispatch\n",
      "    retval = await maybe_coroutine\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/app.py\", line 1144, in _listener\n",
      "    await maybe_coro\n",
      "  File \"/etc/rasa/actions/open-domain-kit/Translate-Service/app.py\", line 10, in load_model\n",
      "    EN_VI_TOKENIZER = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\",\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\", line 484, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 652, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 548, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 636, in _get_config_dict\n",
      "    raise EnvironmentError(\n",
      "OSError: Can't load config for 'Helsinki-NLP/opus-mt-en-vi'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Helsinki-NLP/opus-mt-en-vi' is the correct path to a directory containing a config.json file\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 596, in _get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/utils/hub.py\", line 282, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/utils/hub.py\", line 470, in get_from_cache\n",
      "    os.makedirs(cache_dir, exist_ok=True)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: 'model/en-vi'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/__main__.py\", line 16, in <module>\n",
      "    main()\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/__main__.py\", line 12, in main\n",
      "    cli.run()\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/cli/app.py\", line 84, in run\n",
      "    app.run(**kwargs)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/mixins/runner.py\", line 145, in run\n",
      "    self.__class__.serve(primary=self)  # type: ignore\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/mixins/runner.py\", line 578, in serve\n",
      "    serve_single(primary_server_info.settings)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/server/runners.py\", line 206, in serve_single\n",
      "    serve(**server_settings)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/server/runners.py\", line 130, in serve\n",
      "    loop.run_until_complete(app._server_event(\"init\", \"before\"))\n",
      "  File \"uvloop/loop.pyx\", line 1501, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/app.py\", line 1581, in _server_event\n",
      "    await self.dispatch(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/signals.py\", line 193, in dispatch\n",
      "    return await dispatch\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/signals.py\", line 163, in _dispatch\n",
      "    retval = await maybe_coroutine\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/sanic/app.py\", line 1144, in _listener\n",
      "    await maybe_coro\n",
      "  File \"/etc/rasa/actions/open-domain-kit/Translate-Service/app.py\", line 10, in load_model\n",
      "    EN_VI_TOKENIZER = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-vi\",\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\", line 484, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 652, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 548, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 636, in _get_config_dict\n",
      "    raise EnvironmentError(\n",
      "OSError: Can't load config for 'Helsinki-NLP/opus-mt-en-vi'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Helsinki-NLP/opus-mt-en-vi' is the correct path to a directory containing a config.json file\n",
      "sys:1: RuntimeWarning: coroutine 'Loop.create_server' was never awaited\n"
     ]
    }
   ],
   "source": [
    "!python -m sanic app:app -H 0.0.0.0 -p 5351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opus Machine translation En-Vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-05-10 08:15:42.258793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 08:15:42.258830: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3516: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♪ mọi chuyện là như nhau ♪\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Tokenize text\n",
    "# text = \"Xin chào mọi người, rất vui được gặp mặt!\"\n",
    "text = \"everything is the same\"\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "\n",
    "# Perform translation and decode the output\n",
    "translation = model.generate(**tokenized_text)\n",
    "translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print translated text\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♪ I'm ♪\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "# text = \"Xin chào mọi người, rất vui được gặp mặt!\"\n",
    "text = \"i'm sad\"\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "\n",
    "# Perform translation and decode the output\n",
    "translation = model.generate(**tokenized_text)\n",
    "translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print translated text\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NlpHUST Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/nmt-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-05-17 16:55:56.715903: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-17 16:55:56.715963: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 572/572 [00:00<00:00, 447kB/s]\n",
      "Downloading: 100%|██████████| 1.12G/1.12G [08:08<00:00, 2.46MB/s]  \n",
      "Downloading: 100%|██████████| 4.11M/4.11M [00:26<00:00, 162kB/s] \n",
      "Downloading: 100%|██████████| 98.0/98.0 [00:00<00:00, 49.0kB/s]\n",
      "Downloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 14.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies . \n",
      "--->\t Ở trường, chúng tôi dành nhiều thời gian để nghiên cứu về lịch sử Kim Il-Sung, nhưng chúng tôi chưa bao giờ học được nhiều về thế giới bên ngoài, ngoại trừ Mỹ, Hàn Quốc, Nhật Bản là kẻ thù.\n",
      "\n",
      "\n",
      "I hope you're doing fine \n",
      "--->\t Tôi hi vọng anh đang làm tốt.\n",
      "\n",
      "\n",
      "When the rain keeps falling down. Trouble keeps on coming round \n",
      "--->\t Khi mưa tiếp tục rơi xuống.\n",
      "\n",
      "\n",
      "When the rain keeps falling down, trouble keeps on coming round \n",
      "--->\t Khi mưa tiếp tục rơi xuống, rắc rối tiếp tục đến.\n",
      "\n",
      "\n",
      "If you're hiding what you feel. Underneath a bitter pill \n",
      "--->\t Nếu anh đang che giấu những gì anh cảm thấy.\n",
      "\n",
      "\n",
      "If you're hiding what you feel. underneath a bitter pill \n",
      "--->\t Nếu cô đang che giấu những gì cô cảm thấy. dưới một viên thuốc cay\n",
      "\n",
      "\n",
      "hello \n",
      "--->\t Xin chào.\n",
      "\n",
      "\n",
      "hi \n",
      "--->\t Xin chào.\n",
      "\n",
      "\n",
      "the robots \n",
      "--->\t những con robot\n",
      "\n",
      "\n",
      "the butterflies \n",
      "--->\t những con bướm\n",
      "\n",
      "\n",
      "robots \n",
      "--->\t robots\n",
      "\n",
      "\n",
      "butterfly \n",
      "--->\t Bướm\n",
      "\n",
      "\n",
      "i'm my own boss \n",
      "--->\t tôi là sếp của riêng tôi.\n",
      "\n",
      "\n",
      "i am my own boss \n",
      "--->\t tôi là sếp của riêng tôi.\n",
      "\n",
      "\n",
      "i am ok \n",
      "--->\t tôi ổn.\n",
      "\n",
      "\n",
      "hi, so sweet i can't resist your charm \n",
      "--->\t Xin chào, tôi không thể cưỡng lại sự quyến rũ của cô.\n",
      "\n",
      "\n",
      "don't play with my heart. Before we let it fall too far \n",
      "--->\t Đừng đùa với trái tim tôi trước khi chúng ta để nó rơi quá xa\n",
      "\n",
      "\n",
      "don't play with my heart. before we let it fall too far \n",
      "--->\t Đừng đùa với trái tim tôi trước khi chúng ta để nó rơi quá xa\n",
      "\n",
      "\n",
      "It is a little chilly, but not too bad. how about where you are at? \n",
      "--->\t Nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "It is a little chilly, but not too bad. How about where you are at? \n",
      "--->\t Nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "it is a little chilly, but not too bad. how about where you are at? \n",
      "--->\t nó hơi lạnh, nhưng không quá tệ.\n",
      "\n",
      "\n",
      "hello. i'm doing just fine. how about you? \n",
      "--->\t Xin chào, tôi đang làm rất tốt.\n",
      "\n",
      "\n",
      "hello, i'm doing just fine. How about you? \n",
      "--->\t Xin chào, tôi đang làm tốt.\n",
      "\n",
      "\n",
      "i am right here. But why can't you see. Please scan more! \n",
      "--->\t Tôi đang ở đây, nhưng sao anh không thấy.\n",
      "\n",
      "\n",
      "i am right here. but why can't you see. Please scan more! \n",
      "--->\t Tôi đang ở đây, nhưng sao anh không thấy.\n",
      "\n",
      "\n",
      "One room just ain't enough. But it's hard to leave you alone \n",
      "--->\t Một căn phòng không đủ.\n",
      "\n",
      "\n",
      "One room just ain't enough. but it's hard to leave you alone \n",
      "--->\t Một căn phòng chưa đủ, nhưng khó để cô yên.\n",
      "\n",
      "\n",
      "one room just ain't enough. but it's hard to leave you alone \n",
      "--->\t một căn phòng không đủ, nhưng khó để cô yên.\n",
      "\n",
      "\n",
      "That I know that i'm still free. Be anywhere that I wanna be \n",
      "--->\t Tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "That I know that i'm still free. be anywhere that I wanna be \n",
      "--->\t Tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "that i know that i'm still free. be anywhere that i wanna be \n",
      "--->\t mà tôi biết rằng tôi vẫn còn tự do.\n",
      "\n",
      "\n",
      "Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. When we are apart \n",
      "--->\t Khoảng cách khiến trái tim phát triển. hạnh phúc khi biết rằng tình yêu của anh chưa bao giờ đi xa.\n",
      "\n",
      "\n",
      "Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart \n",
      "--->\t Khoảng cách khiến trái tim phát triển. thật hạnh phúc khi biết rằng tình yêu của anh không bao giờ xa, khi chúng ta chia cắt\n",
      "\n",
      "\n",
      "distance makes the heart grows. even when i'm lonely, happy knowing that your love is never far. when we are apart \n",
      "--->\t khoảng cách làm tim phát triển. thậm chí khi tôi cô đơn, hạnh phúc khi biết rằng tình yêu của anh chưa bao giờ đi xa.\n",
      "\n",
      "\n",
      "When you love. It makes a heart. In the middle of a fight. Walk away to make it right \n",
      "--->\t Khi em yêu, nó là một trái tim.\n",
      "\n",
      "\n",
      "When you love, It makes a heart. In the middle of a fight; Walk away to make it right \n",
      "--->\t Khi em yêu, nó là một trái tim.\n",
      "\n",
      "\n",
      "when you love, it makes a heart. in the middle of a fight; walk away to make it right \n",
      "--->\t khi em yêu, nó là một trái tim. giữa cuộc chiến, bước đi để làm cho nó đúng\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-en-vi-small\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-en-vi-small\")\n",
    "t5_model.to(device)\n",
    "\n",
    "srcs = [\"In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\",\n",
    "       \"I hope you're doing fine\",\n",
    "       \"When the rain keeps falling down. Trouble keeps on coming round\",\n",
    "       \"When the rain keeps falling down, trouble keeps on coming round\",\n",
    "       \"If you're hiding what you feel. Underneath a bitter pill\",\n",
    "       \"If you're hiding what you feel. underneath a bitter pill\",\n",
    "       \"hello\", \"hi\", \"the robots\",\n",
    "       \"the butterflies\", \"robots\", \"butterfly\",\n",
    "       \"i'm my own boss\", \"i am my own boss\",\n",
    "       \"i am ok\",\n",
    "       \"hi, so sweet i can't resist your charm\",\n",
    "       \"don't play with my heart. Before we let it fall too far\",\n",
    "       \"don't play with my heart. before we let it fall too far\",\n",
    "       \"It is a little chilly, but not too bad. how about where you are at?\",\n",
    "       \"It is a little chilly, but not too bad. How about where you are at?\",\n",
    "       \"It is a little chilly, but not too bad. how about Where you are at?\".lower(),\n",
    "       \"hello. i'm doing just fine. how about you?\",\n",
    "       \"hello, i'm doing just fine. How about you?\",\n",
    "       \"i am right here. But why can't you see. Please scan more!\",\n",
    "       \"i am right here. but why can't you see. Please scan more!\",\n",
    "       \"One room just ain't enough. But it's hard to leave you alone\",\n",
    "       \"One room just ain't enough. but it's hard to leave you alone\",\n",
    "       \"One room just ain't enough. but it's hard to leave you alone\".lower(),\n",
    "       \"That I know that i'm still free. Be anywhere that I wanna be\",\n",
    "       \"That I know that i'm still free. be anywhere that I wanna be\",\n",
    "       \"That I know that i'm still free. be anywhere that I wanna be\".lower(),\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. When we are apart\",\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart\",\n",
    "       \"Distance makes the heart grows. Even when I'm lonely, happy knowing that your love is never far. when we are apart\".lower(),\n",
    "       \"When you love. It makes a heart. In the middle of a fight. Walk away to make it right\",\n",
    "       \"When you love, It makes a heart. In the middle of a fight; Walk away to make it right\",\n",
    "       \"When you love, It makes a heart. In the middle of a fight; Walk away to make it right\".lower(),]\n",
    "for src in srcs:\n",
    "    tokenized_text = t5_tokenizer.encode(src, return_tensors=\"pt\").to(device)\n",
    "    t5_model.eval()\n",
    "    summary_ids = t5_model.generate(\n",
    "                        tokenized_text,\n",
    "                        max_length=128, \n",
    "                        num_beams=5,\n",
    "                        repetition_penalty=2.5, \n",
    "                        length_penalty=1.0, \n",
    "                        early_stopping=True)\n",
    "    output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(f\"{src} \\n--->\\t {output}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 607/607 [00:00<00:00, 301kB/s]\n",
      "Downloading: 100%|██████████| 1.12G/1.12G [06:05<00:00, 3.29MB/s]   \n",
      "Downloading: 100%|██████████| 4.11M/4.11M [00:02<00:00, 1.68MB/s]\n",
      "Downloading: 100%|██████████| 98.0/98.0 [00:00<00:00, 22.4kB/s]\n",
      "Downloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 23.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin chào ngày mới. thời tiết ở đó thế nào?  ----> \t Hello, new day. How's the weather there?\n",
      "Indonesia phỏng đoán nguyên nhân tàu ngầm chở 53 người mất tích bí ẩn  ----> \t Indonesia anticipates the cause of the submarine transporting 53 mysterious missing persons\n",
      "bạn khỏe không? Rất vui được gặp mặt!  ----> \t How are you?\n",
      "bạn có khỏe không? rất vui được gặp lại bạn!  ----> \t How are you? I'm so happy to see you again!\n",
      "bạn khỏe không? rất vui được gặp mặt!  ----> \t How are you? I'm so happy to meet you!\n",
      "bạn khỏe không? Rất vui được gặp mặt!  ----> \t How are you?\n"
     ]
    }
   ],
   "source": [
    "t5_model_vi = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-vi-en-small\")\n",
    "t5_tokenizer_vi = T5Tokenizer.from_pretrained(\"NlpHUST/t5-vi-en-small\")\n",
    "t5_model_vi.to(device)\n",
    "\n",
    "srcs = [\"xin chào ngày mới. thời tiết ở đó thế nào?\",\n",
    "       \"Indonesia phỏng đoán nguyên nhân tàu ngầm chở 53 người mất tích bí ẩn\",\n",
    "       \"bạn khỏe không? Rất vui được gặp mặt!\",\n",
    "       \"bạn có khỏe không? rất vui được gặp lại bạn!\",\n",
    "       \"bạn khỏe không? rất vui được gặp mặt!\",\n",
    "       \"bạn khỏe không? Rất vui được gặp mặt!\"]\n",
    "for src in srcs:\n",
    "    tokenized_text = t5_tokenizer_vi.encode(src, return_tensors=\"pt\").to(device)\n",
    "    t5_model_vi.eval()\n",
    "    summary_ids = t5_model_vi.generate(\n",
    "                        tokenized_text,\n",
    "                        max_length=256, \n",
    "                        num_beams=5,\n",
    "                        repetition_penalty=2.5, \n",
    "                        length_penalty=1.0, \n",
    "                        early_stopping=True\n",
    "                    )\n",
    "    output = t5_tokenizer_vi.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(src, \" ----> \\t\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sanic Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 service blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url_obj = {'text': \"xin chào, hôm nay thời tiết như thế nào?\"}\n",
    "# url_obj = {'text': \"so no one told you life was gonna be this way?\"}\n",
    "\n",
    "stt_server = 'http://0.0.0.0:8002/vi-to-en'\n",
    "# stt_server = 'http://0.0.0.0:8002/en-to-vi'\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "vi_en = text.text\n",
    "# print(vi_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is a little chilly, but not too bad. How about where you are at?\n"
     ]
    }
   ],
   "source": [
    "stt_server = 'http://0.0.0.0:8001/open-chat'\n",
    "text_obj = {'text': vi_en}\n",
    "text = requests.post(stt_server, data=json.dumps(text_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "print(bot_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"i'm my own boss\"}\n",
      "♪ ♪ I'm my master ♪\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# url_obj = {'text': \"xin chào, hôm nay thời tiết như thế nào?\"}\n",
    "# url_obj = {'text': \" It is a little chilly, but not too bad. How about where you are at?\"}\n",
    "url_obj = {'text': \"i'm my own boss\"}\n",
    "print(url_obj)\n",
    "# stt_server = 'http://0.0.0.0:8002/vi-to-en'\n",
    "stt_server = 'http://0.0.0.0:8001/en-to-vi'\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "en_vi = text.text\n",
    "print(en_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 blocks connected into 1 flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t###EN-TO-VI\n",
      "[{'text': 'cung hoàng đạo của bạn là gì'}, {'text': '?'}]\n",
      "What is your horoscope?? \n",
      "\t###BLENDER-BOT\n",
      "i don't have a horoscope. i'm not sure what it is.\n",
      "\t###EN-TO-VI\n",
      "[{'text': \"i don't have a horoscope\"}, {'text': '.'}, {'text': \" i'm not sure what it is\"}, {'text': '.'}]\n",
      "tôi không có tử vi. tôi không chắc nó là gì. \n"
     ]
    }
   ],
   "source": [
    "# COMPLETE FLOW\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"\\t###EN-TO-VI\")\n",
    "msg = \"cung hoàng đạo của bạn là gì?\"\n",
    "# url_objs = [{'text': re.sub(r\"[\\?\\!\\.]\", \",\", msg.lower())}]\n",
    "# url_objs = [{'text': msg.lower()}]\n",
    "url_objs = []\n",
    "for i in re.split(\"([,\\?\\!\\.])\", msg.lower()):\n",
    "    if len(i) > 0:\n",
    "        url_objs.append({'text': i})\n",
    "print(url_objs)\n",
    "stt_server = 'http://0.0.0.0:8001/vi-to-en'\n",
    "vi_en = \"\"\n",
    "for i, url_obj in enumerate(url_objs):\n",
    "    if url_obj[\"text\"] not in [\",\", \"?\", \"!\", \".\"]:\n",
    "        text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "        vi_en += text.text\n",
    "    else:\n",
    "        vi_en += url_obj[\"text\"] + \" \"\n",
    "print(vi_en)\n",
    "\n",
    "print(\"\\t###BLENDER-BOT\")\n",
    "stt_server = 'http://0.0.0.0:8002/open-chat'\n",
    "url_obj = {'text': re.sub(r\"[\\?\\!\\.]\", \",\", vi_en.lower())}\n",
    "# url_objs = [{'text': vi_en.lower()}]\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "print(bot_answer)\n",
    "\n",
    "\n",
    "print(\"\\t###EN-TO-VI\")\n",
    "# url_objs = [{'text': re.sub(r\"[\\?\\!\\.]\", \",\", bot_answer.lower())}]\n",
    "# url_objs = [{'text': bot_answer.lower()}]\n",
    "url_objs = []\n",
    "for i in re.split(\"([,\\?\\!\\.])\", bot_answer.lower()):\n",
    "    if len(i) > 0:\n",
    "        url_objs.append({'text': i})\n",
    "print(url_objs)\n",
    "\n",
    "stt_server = 'http://0.0.0.0:8001/en-to-vi'\n",
    "en_vi = \"\"\n",
    "for i, url_obj in enumerate(url_objs):\n",
    "    if url_obj[\"text\"] not in [\",\", \"?\", \"!\", \".\"]:\n",
    "        text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "        en_vi += text.text\n",
    "    else:\n",
    "        en_vi += url_obj[\"text\"] + \" \"\n",
    "print(en_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm sorry to hear that, why do you want to kill yourself, what's wrong,\n"
     ]
    }
   ],
   "source": [
    "_bot_answer = re.sub(r\"[\\?\\!\\.]\", \",\", bot_answer)\n",
    "print(_bot_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'text': \"xin chào một ngày tốt lành\".lower()}\n",
    "\n",
    "\n",
    "text = requests.post('http://0.0.0.0:8001/vi-to-en', data=json.dumps(payload))\n",
    "vi_en = text.text\n",
    "\n",
    "text_obj = {'text': vi_en.lower()}\n",
    "text = requests.post('http://0.0.0.0:8002/open-chat', data=json.dumps(text_obj))\n",
    "bot_answer = text.text[9:-2]\n",
    "\n",
    "text_obj = {'text': bot_answer.lower()}\n",
    "text = requests.post('http://0.0.0.0:8001/en-to-vi', data=json.dumps(text_obj))\n",
    "en_vi = text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanic service for NlpHUST Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi_en:  Hello, how's the weather today?\n",
      "en_vi: Tôi là sếp của tôi, tôi đã làm một dự án mới từ lâu rồi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Vi to En\n",
    "url_obj = {'text': \"xin chào, hôm nay thời tiết như thế nào?\"}\n",
    "stt_server = 'http://0.0.0.0:8001/vi-to-en'\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "vi_en = text.text\n",
    "print(\"vi_en: \", vi_en)\n",
    "\n",
    "\n",
    "# En to Vi\n",
    "url_obj = {'text': \"i'm my own boss. i've been working on a new project for a while now\"}\n",
    "stt_server = 'http://0.0.0.0:8001/en-to-vi'\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "en_vi = text.text\n",
    "print(\"en_vi:\", en_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Rasa chatbot eqquiped with Open-domain-kit via REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'test', 'text': 'Lúc nào cũng vui khi có cơ hội ngắm nhìn thời tiết tốt nhất thế giới.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "payload = {\"sender\": \"test\", \"message\": \"dự báo thời tiết hôm nay\"}\n",
    "\n",
    "text = requests.post('http://localhost:5005/webhooks/rest/webhook', data=json.dumps(payload))\n",
    "vi_en = text.text\n",
    "print(json.loads(vi_en))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2b9313a5ac1dd114cb80c590fd5994e6bb5ba093af05652baffc6e4ba6c0185"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('nmt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
