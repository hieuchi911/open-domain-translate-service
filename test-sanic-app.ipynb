{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch              1.11.0   \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.18.0\n",
    "# !pip install sentencepiece\n",
    "# !pip install sanic==22.3.1\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/Documents/CT Group/chatbot and NLP/Translate-Service/t-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3516: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em là dung nham của em, em là của anh. chúng ta hôn nhau và nụ hôn của chúng ta đi khắp mọi nơi\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/opus-mt-en-vi\")\n",
    "\n",
    "# Tokenize text\n",
    "# text = \"Xin chào mọi người, rất vui được gặp mặt!\"\n",
    "text = \"i'm your lava, you're my lava. we kiss and our kiss goes everywhere\"\n",
    "tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "\n",
    "# Perform translation and decode the output\n",
    "translation = model.generate(**tokenized_text)\n",
    "translated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print translated text\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sanic Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"V\\u1eady l\\u00e0 kh\\u00f4ng ai n\\u00f3i v\\u1edbi anh l\\u00e0 cu\\u1ed9c s\\u1ed1ng c\\u1ee7a anh s\\u1ebd nh\\u01b0 th\\u1ebf n\\u00e0y?\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://127.0.0.1:8000'\n",
    "\n",
    "# url_obj = {'text': \"xin chào, tôi là trợ lí ảo đến từ tập đoàn CT Group\"}\n",
    "url_obj = {'text': \"so no one told you life was gonna be this way?\"}\n",
    "\n",
    "# stt_server = 'http://0.0.0.0:5351/vi-to-en'\n",
    "stt_server = 'http://0.0.0.0:5351/en-to-vi'\n",
    "\n",
    "text = requests.post(stt_server, data=json.dumps(url_obj))\n",
    "print(text.text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2b9313a5ac1dd114cb80c590fd5994e6bb5ba093af05652baffc6e4ba6c0185"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('nmt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
